{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_hyperparameters_presentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "D65dbB7dL6PA",
        "-awdNo2UL6PG",
        "yh8wI2z7Oeau"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D65dbB7dL6PA",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Optimization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bqmyHVbL6PD",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters (HP) are those parameters that machine or deep learning algorithms cannot learn by training. The value of those parameters needs to be set before the training process and it can control how the algorithm learns from the data. Hyperparameters (HP) can be the number of layers or a neural network or number of nodes in a layer, it can be the learning rate or the type of optimizer that the neural network can use. Getting the right hyperparameters can be counter-intuitive, it is more of a trial and error process. Hence, we have the need to find a way to automatically tune those parameters to get the best combination that will maximize the ML model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLeYNZhCL6PE",
        "colab_type": "text"
      },
      "source": [
        "###Objective Function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNjW6P5YL6PF",
        "colab_type": "text"
      },
      "source": [
        "Machine Learning can be simply defined as learning behaviour from experience. By learning here, we mean getting better and improving at some certain task over time. But what constitutes an improvement?\n",
        "\n",
        "In order to develop a function to judge the improvement, we need to have formal measures of how good or bad our models are. In machine learning, we call these “objective functions”. The most common objective function is squared error which is basically the difference between the predicted value and the actual ground truth. If this value is large, then the accuracy is low. Our target from optimizing the hyperparameters will be to minimize this objective function. Getting the right combination of the model’s hyperparameters can contribute largely into the model’s performance, Let’s have a look at how we can tune those parameters to minimize the objective function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-awdNo2UL6PG",
        "colab_type": "text"
      },
      "source": [
        "## 1- Grid Search:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR3bjdWAL6PH",
        "colab_type": "text"
      },
      "source": [
        "![Search Grid](https://raw.githubusercontent.com/quantumhack/data/40822efd2f3467e6985909c96b5501313fcb5d46/Projects/Grid.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg7CvxyjL6PI",
        "colab_type": "text"
      },
      "source": [
        "One way of tuning the hyperparameters is to define an equal range for each HP and have the computer try all possible combinations of parameter values. It can be a great approach to get the best combination of HPs. It works by putting the HPs that you want to tune in an n-dimensional grid then try each and every combination in this grid. Let’s have a look at how it works, the example below will try to find the best combination of hyperparameters to build a classifier for the MNIST dataset.\n",
        "\n",
        "The code above will brute force and try every possible combination of those three hyper-parameters then print out the best combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9bn576UN9pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWewIFz6OqPL",
        "colab_type": "code",
        "outputId": "42ef08fd-2273-4cf5-8ab6-1708ab400eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#load the MNIST dataset and normalize it\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#we are going to evaluate the number of hidden layers (3 cases) and \n",
        "#hidden units (3 cases)\n",
        "hidden_layers_ = [1, 2, 3]\n",
        "hidden_units_ = [5, 10, 15]\n",
        "#also we are going to evaluate the best solver (2 cases) and the \n",
        "#learning rate (3 cases)\n",
        "solver_ = ['sgd', 'adam']\n",
        "lr_ = [0.001, 0.01, 0.1]\n",
        "\n",
        "#we set some constant values which could also be tunned but for the sake of \n",
        "#making the execution shorter we do not\n",
        "epochs_ = 10\n",
        "batch_size_ = 32\n",
        "momentum_ = 0.9\t\n",
        "nesterov_ = True\n",
        "\n",
        "#set some constants for the algorithm\n",
        "test_loss = [0] * (len(hidden_units_) * len(hidden_layers_) * len(solver_) * len(lr_))\n",
        "test_acc =  [0] * (len(hidden_units_) * len(hidden_layers_) * len(solver_) * len(lr_))\n",
        "caso = 0\n",
        "max_acc = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVsZl9EmP3hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we run the algorithms for every combination of every case and later we select\n",
        "#the one with the best accuracy\n",
        "\n",
        "for i in range(len(hidden_units_)):\n",
        "  for j in range(len(hidden_layers_)):\n",
        "    for k in range(len(lr_)):\n",
        "      for l in range(len(solver_)):\n",
        "        model = Sequential()\n",
        "        model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "        model.add(layers.Dense(hidden_units_[i], input_shape=(28, 28), activation='relu'))\n",
        "        for i in range(hidden_layers_[j] - 1):\n",
        "          model.add(layers.Dense(hidden_units_[i], activation='relu'))\n",
        "        model.add(layers.Dense(10,activation='softmax'))\n",
        "        \n",
        "        if solver_[l] == 'sgd':\n",
        "          optimizer = optimizers.SGD(lr=lr_[k], momentum = momentum_, nesterov=nesterov_)\n",
        "        if solver_[l] == 'Adam':\n",
        "          optimizer = optimizers.Adam(lr=lr_[k])\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        history = model.fit(x_train, y_train, epochs=epochs_, verbose=0, batch_size=batch_size_, validation_data=(x_test, y_test))\n",
        "        test_loss[caso], test_acc[caso] = model.evaluate(x_test, y_test, verbose=0)  \n",
        "        if test_acc[caso]>max_acc:\n",
        "          max_acc = test_acc[caso]\n",
        "          neu_opt = hidden_units_[i]\n",
        "          lay_opt = hidden_layers_[j]\n",
        "          lra_opt = lr_[k]\n",
        "          sol_opt = solver_[l]\n",
        "          hist_opt = history\n",
        "        \n",
        "        caso += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrsNWS47SMTH",
        "colab_type": "code",
        "outputId": "e23d9854-b9e5-49ab-a3d4-296f19122306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "source": [
        "#then we print the accuracy for every combination we tried\n",
        "#and later print the best case\n",
        "caso = 0\n",
        "for i in range(len(hidden_units_)):\n",
        "\tfor j in range(len(hidden_layers_)):\n",
        "\t\tfor k in range(len(lr_)):\n",
        "\t\t\tfor l in range(len(solver_)):\n",
        "\t\t\t\tprint(\"Cantidad de layers\",hidden_layers_[j],\". Cantidad de neuronas\", hidden_units_[i],\". Solver:\",solver_[l],\". Learning rate:\", lr_[k],\". Accuracy:\", test_acc[caso])\n",
        "\t\t\t\tcaso += 1\t\t\t\t\n",
        "print(\"---------------------------------------------\")\n",
        "print(\"El Accuracy máximo es:\",max_acc,\". Para el caso con\",lay_opt,\" layers,\",neu_opt,\"cantidad de neuronas, \",sol_opt,\" y un learning rate de\",lra_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.8885999917984009\n",
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.8787000179290771\n",
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.8532999753952026\n",
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.8877000212669373\n",
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.7152000069618225\n",
            "Cantidad de layers 1 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.7678999900817871\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.8863999843597412\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.8636999726295471\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.8895000219345093\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.8895999789237976\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.30230000615119934\n",
            "Cantidad de layers 2 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.20909999310970306\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.8809000253677368\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.9222000241279602\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.9287999868392944\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.9327999949455261\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.10090000182390213\n",
            "Cantidad de layers 3 . Cantidad de neuronas 5 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.1996999979019165\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.9240999817848206\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.923799991607666\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.9409000277519226\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.934499979019165\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.7810999751091003\n",
            "Cantidad de layers 1 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.909600019454956\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.8892999887466431\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.8664000034332275\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.8932999968528748\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.8751999735832214\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.1624000072479248\n",
            "Cantidad de layers 2 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.288100004196167\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.866599977016449\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.9110999703407288\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.9345999956130981\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.9311000108718872\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.2069000005722046\n",
            "Cantidad de layers 3 . Cantidad de neuronas 10 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.1826999932527542\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.9345999956130981\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.9282000064849854\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.9517999887466431\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.9473999738693237\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.9398999810218811\n",
            "Cantidad de layers 1 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.9265999794006348\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.9232000112533569\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.8672000169754028\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.8823999762535095\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.8841999769210815\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.11349999904632568\n",
            "Cantidad de layers 2 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.11429999768733978\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.001 . Accuracy: 0.8761000037193298\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.001 . Accuracy: 0.909500002861023\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.01 . Accuracy: 0.9279000163078308\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.01 . Accuracy: 0.9194999933242798\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: sgd . Learning rate: 0.1 . Accuracy: 0.10279999673366547\n",
            "Cantidad de layers 3 . Cantidad de neuronas 15 . Solver: adam . Learning rate: 0.1 . Accuracy: 0.10090000182390213\n",
            "---------------------------------------------\n",
            "El Accuracy máximo es: 0.9517999887466431 . Para el caso con 1  layers, 15 cantidad de neuronas,  sgd  y un learning rate de 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_JMumSzL6PU",
        "colab_type": "text"
      },
      "source": [
        "## 2- Bayesian Optimization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiuXkzPjL6PU",
        "colab_type": "text"
      },
      "source": [
        "![Bayesian Optimization:](https://raw.githubusercontent.com/quantumhack/data/40822efd2f3467e6985909c96b5501313fcb5d46/Projects/Bayes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AaY7A1FL6PW",
        "colab_type": "text"
      },
      "source": [
        "In the above examples, we had an objective metric which is the accuracy and we also had an objective function that tries to maximize the metric and minimize the loss. Bayesian Optimization approach tries to find the value that minimizes an objective function by building a probability model based on past evaluation results of the objective metric. It tries to find the balance between the exploration of the best region that contains the best hyperparameters and the exploitation of this region to maximize/minimize the objective metric.\n",
        "\n",
        "The problem of optimizing hyper-parameters is that it is an expensive process to assess the performance of a set of HPs because we have to build the corresponding graphs or neural networks in each iteration, then we have to train it, and finally, we have to assess the performance. The optimization process can take hours or days but in this example, we will train on only 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVDWY9lAL6PX",
        "colab_type": "text"
      },
      "source": [
        "Let’s have a look at the example below that uses Bayesian Optimization to tune a neural network for the MNIST dataset. We will use scikit-optimize library to perform the HPO. It’s one of the libraries that has implemented the Bayesian Optimization algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_nxchA-TIDy",
        "colab_type": "code",
        "outputId": "4b72a4a6-ae6f-48fe-cdf4-069c42bea9e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/87/310b52debfbc0cb79764e5770fa3f5c18f6f0754809ea9e2fc185e1b67d3/scikit_optimize-0.7.4-py2.py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.1MB/s \n",
            "\u001b[?25hCollecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.14.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2yGdfcDTLFT",
        "colab_type": "code",
        "outputId": "b6912455-6b55-4b3d-b3b0-b3beafd8da54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "import skopt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam, SGD\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.framework import ops\n",
        "from skopt import gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiWrZIDjTQp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we load the dataset, normalize and converto the y outout to categorical\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train/ 255\n",
        "X_test = X_test/ 255\n",
        "X_train = X_train.reshape(60000,784)\n",
        "X_test = X_test.reshape(10000,784)\n",
        "input_shape= X_train[0].shape\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "#we are going to evaluate the same parameters that we evaluate in the case before\n",
        "# number of layer, number of units, optimizer and learning rate\n",
        "dim_num_dense_layers = Integer(low=0, high=2, name='num_dense_layers')\n",
        "dim_num_dense_nodes = Integer(low=5, high=15, name='num_dense_nodes')\n",
        "dim_optimizer = Categorical(categories=['Adam', 'SGD'], name='optimizer')\n",
        "dim_learning_rate = Real(low=1e-3, high=1e-1, prior='log-uniform', name='learning_rate')\n",
        "\n",
        "#we set the values for the bayesian optimizer to train the neural network\n",
        "dimensions = [dim_learning_rate,\n",
        "              dim_num_dense_layers,\n",
        "              dim_num_dense_nodes,\n",
        "              dim_optimizer]\n",
        "default_parameters = [1e-3, 2, 13, 'SGD']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO5GrHxcUH_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's define the model depending on the values for each call\n",
        "def create_model(learning_rate, num_dense_layers, num_dense_nodes, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_dense_nodes, input_shape= input_shape, activation='relu'))\n",
        "    for i in range(num_dense_layers):\n",
        "        name = 'layer_dense_{0}'.format(i+1)\n",
        "        model.add(Dense(num_dense_nodes, activation='relu', name=name))\n",
        "    model.add(Dense(10,activation='softmax'))\n",
        "    if optimizer == 'Adam':\n",
        "      optimizer_final = Adam(lr=learning_rate)\n",
        "    if optimizer == 'SGD':\n",
        "      optimizer_final = SGD(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer_final, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#let's define the fitness depending of the values for each call\n",
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(learning_rate, num_dense_layers, num_dense_nodes, optimizer):\n",
        "\n",
        "    model = create_model(learning_rate=learning_rate,\n",
        "                         num_dense_layers=num_dense_layers,\n",
        "                         num_dense_nodes=num_dense_nodes,\n",
        "                         optimizer=optimizer)    \n",
        "\n",
        "    blackbox = model.fit(x=X_train, y=y_train, epochs=10, batch_size=32, validation_split=0.15,)\n",
        "    accuracy = blackbox.history['val_accuracy'][-1]\n",
        "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
        "\n",
        "    del model\n",
        "    K.clear_session()\n",
        "    ops.reset_default_graph()    \n",
        "\n",
        "    return -accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2WdQYVzUequ",
        "colab_type": "code",
        "outputId": "a506e411-eed0-42c1-fac5-f52e1253988e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session()\n",
        "ops.reset_default_graph()\n",
        "\n",
        "#call the bayesian optimizer\n",
        "gp_result = gp_minimize(func=fitness, dimensions=dimensions, n_calls=12, n_jobs=-1, kappa = 5, x0=default_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 3s 51us/step - loss: 2.2942 - accuracy: 0.1146 - val_loss: 2.2701 - val_accuracy: 0.1523\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 2.2145 - accuracy: 0.2006 - val_loss: 2.1361 - val_accuracy: 0.2864\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 2.0350 - accuracy: 0.3131 - val_loss: 1.9064 - val_accuracy: 0.3551\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 1.7881 - accuracy: 0.4057 - val_loss: 1.6322 - val_accuracy: 0.4699\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 1.5326 - accuracy: 0.5096 - val_loss: 1.3870 - val_accuracy: 0.5906\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 1.3214 - accuracy: 0.6203 - val_loss: 1.1834 - val_accuracy: 0.6858\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 1.1265 - accuracy: 0.6959 - val_loss: 0.9876 - val_accuracy: 0.7391\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.9419 - accuracy: 0.7406 - val_loss: 0.8188 - val_accuracy: 0.7738\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.7995 - accuracy: 0.7723 - val_loss: 0.7016 - val_accuracy: 0.8036\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.7065 - accuracy: 0.7945 - val_loss: 0.6243 - val_accuracy: 0.8234\n",
            "Accuracy: 82.34%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.6060 - accuracy: 0.8089 - val_loss: 0.4787 - val_accuracy: 0.8641\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4858 - accuracy: 0.8570 - val_loss: 0.4549 - val_accuracy: 0.8699\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4690 - accuracy: 0.8616 - val_loss: 0.4233 - val_accuracy: 0.8719\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4603 - accuracy: 0.8666 - val_loss: 0.4116 - val_accuracy: 0.8836\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4507 - accuracy: 0.8714 - val_loss: 0.3954 - val_accuracy: 0.8842\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4503 - accuracy: 0.8708 - val_loss: 0.4072 - val_accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4379 - accuracy: 0.8754 - val_loss: 0.3956 - val_accuracy: 0.8892\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4354 - accuracy: 0.8755 - val_loss: 0.4149 - val_accuracy: 0.8837\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4289 - accuracy: 0.8794 - val_loss: 0.4268 - val_accuracy: 0.8794\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4357 - accuracy: 0.8770 - val_loss: 0.3877 - val_accuracy: 0.8900\n",
            "Accuracy: 89.00%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.4727 - accuracy: 0.8560 - val_loss: 0.3883 - val_accuracy: 0.8843\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.2936 - accuracy: 0.9145 - val_loss: 0.2586 - val_accuracy: 0.9260\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.2581 - accuracy: 0.9253 - val_loss: 0.2217 - val_accuracy: 0.9336\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.2395 - accuracy: 0.9285 - val_loss: 0.2091 - val_accuracy: 0.9416\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.2241 - accuracy: 0.9341 - val_loss: 0.2834 - val_accuracy: 0.9217\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.2157 - accuracy: 0.9357 - val_loss: 0.1954 - val_accuracy: 0.9410\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.2064 - accuracy: 0.9390 - val_loss: 0.1968 - val_accuracy: 0.9427\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.2012 - accuracy: 0.9403 - val_loss: 0.2018 - val_accuracy: 0.9397\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.1968 - accuracy: 0.9406 - val_loss: 0.2006 - val_accuracy: 0.9437\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.1911 - accuracy: 0.9436 - val_loss: 0.1961 - val_accuracy: 0.9450\n",
            "Accuracy: 94.50%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.7290 - accuracy: 0.7637 - val_loss: 0.5249 - val_accuracy: 0.8376\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.5206 - accuracy: 0.8433 - val_loss: 0.4750 - val_accuracy: 0.8571\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4816 - accuracy: 0.8576 - val_loss: 0.4297 - val_accuracy: 0.8751\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.4504 - accuracy: 0.8696 - val_loss: 0.4060 - val_accuracy: 0.8851\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.4447 - accuracy: 0.8701 - val_loss: 0.3987 - val_accuracy: 0.8821\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4325 - accuracy: 0.8739 - val_loss: 0.4331 - val_accuracy: 0.8751\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 0.4270 - accuracy: 0.8762 - val_loss: 0.3730 - val_accuracy: 0.8904\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4253 - accuracy: 0.8761 - val_loss: 0.4011 - val_accuracy: 0.8848\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.4195 - accuracy: 0.8773 - val_loss: 0.4326 - val_accuracy: 0.8779\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4149 - accuracy: 0.8800 - val_loss: 0.3957 - val_accuracy: 0.8846\n",
            "Accuracy: 88.46%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 3s 56us/step - loss: 0.5370 - accuracy: 0.8482 - val_loss: 0.3478 - val_accuracy: 0.9130\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 3s 52us/step - loss: 0.4083 - accuracy: 0.8944 - val_loss: 0.3799 - val_accuracy: 0.8854\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 3s 55us/step - loss: 0.3825 - accuracy: 0.9017 - val_loss: 0.3618 - val_accuracy: 0.9083\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 3s 52us/step - loss: 0.4352 - accuracy: 0.8886 - val_loss: 0.4284 - val_accuracy: 0.8879\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 3s 52us/step - loss: 0.4136 - accuracy: 0.8927 - val_loss: 0.4037 - val_accuracy: 0.8910\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 3s 55us/step - loss: 0.4284 - accuracy: 0.8922 - val_loss: 0.3238 - val_accuracy: 0.9183\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 3s 56us/step - loss: 0.3922 - accuracy: 0.8999 - val_loss: 0.3174 - val_accuracy: 0.9202\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 3s 56us/step - loss: 0.3951 - accuracy: 0.9007 - val_loss: 0.3544 - val_accuracy: 0.9136\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 3s 54us/step - loss: 0.4411 - accuracy: 0.8897 - val_loss: 0.4525 - val_accuracy: 0.8866\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 0.4654 - accuracy: 0.8828 - val_loss: 0.3927 - val_accuracy: 0.9089\n",
            "Accuracy: 90.89%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.5991 - accuracy: 0.8374 - val_loss: 0.4352 - val_accuracy: 0.8884\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.5492 - accuracy: 0.8647 - val_loss: 0.5066 - val_accuracy: 0.8860\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.5501 - accuracy: 0.8678 - val_loss: 0.4853 - val_accuracy: 0.8859\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.5537 - accuracy: 0.8700 - val_loss: 0.5880 - val_accuracy: 0.8660\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.5585 - accuracy: 0.8681 - val_loss: 0.5461 - val_accuracy: 0.8698\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.5592 - accuracy: 0.8671 - val_loss: 0.5221 - val_accuracy: 0.8981\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.5646 - accuracy: 0.8678 - val_loss: 0.5354 - val_accuracy: 0.8856\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.5515 - accuracy: 0.8686 - val_loss: 0.6474 - val_accuracy: 0.8734\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.5589 - accuracy: 0.8710 - val_loss: 0.6277 - val_accuracy: 0.8830\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.5574 - accuracy: 0.8698 - val_loss: 0.6355 - val_accuracy: 0.8848\n",
            "Accuracy: 88.48%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.8066 - accuracy: 0.7335 - val_loss: 0.4510 - val_accuracy: 0.8639\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.4795 - accuracy: 0.8556 - val_loss: 0.4021 - val_accuracy: 0.8780\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.4294 - accuracy: 0.8711 - val_loss: 0.3987 - val_accuracy: 0.8794\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.3999 - accuracy: 0.8818 - val_loss: 0.3221 - val_accuracy: 0.9050\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.3741 - accuracy: 0.8881 - val_loss: 0.3159 - val_accuracy: 0.9078\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.3564 - accuracy: 0.8952 - val_loss: 0.3012 - val_accuracy: 0.9120\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.3480 - accuracy: 0.8973 - val_loss: 0.4563 - val_accuracy: 0.8652\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.3373 - accuracy: 0.9013 - val_loss: 0.2984 - val_accuracy: 0.9142\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 42us/step - loss: 0.3279 - accuracy: 0.9043 - val_loss: 0.2752 - val_accuracy: 0.9193\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.3196 - accuracy: 0.9069 - val_loss: 0.2802 - val_accuracy: 0.9186\n",
            "Accuracy: 91.86%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 1.8921 - accuracy: 0.3504 - val_loss: 1.4312 - val_accuracy: 0.5621\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 41us/step - loss: 1.1966 - accuracy: 0.6472 - val_loss: 0.9671 - val_accuracy: 0.7140\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 41us/step - loss: 0.9018 - accuracy: 0.7325 - val_loss: 0.7491 - val_accuracy: 0.7834\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 43us/step - loss: 0.7307 - accuracy: 0.7867 - val_loss: 0.6114 - val_accuracy: 0.8229\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 41us/step - loss: 0.6252 - accuracy: 0.8191 - val_loss: 0.5322 - val_accuracy: 0.8449\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 42us/step - loss: 0.5534 - accuracy: 0.8409 - val_loss: 0.4734 - val_accuracy: 0.8632\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 42us/step - loss: 0.4992 - accuracy: 0.8570 - val_loss: 0.4258 - val_accuracy: 0.8758\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 41us/step - loss: 0.4595 - accuracy: 0.8682 - val_loss: 0.3981 - val_accuracy: 0.8853\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 42us/step - loss: 0.4321 - accuracy: 0.8770 - val_loss: 0.3774 - val_accuracy: 0.8909\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 40us/step - loss: 0.4133 - accuracy: 0.8813 - val_loss: 0.3664 - val_accuracy: 0.8944\n",
            "Accuracy: 89.44%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.8429 - accuracy: 0.7195 - val_loss: 0.6364 - val_accuracy: 0.8131\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.6179 - accuracy: 0.8191 - val_loss: 0.5882 - val_accuracy: 0.8314\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.5764 - accuracy: 0.8312 - val_loss: 0.5468 - val_accuracy: 0.8346\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.5612 - accuracy: 0.8372 - val_loss: 0.4962 - val_accuracy: 0.8541\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 0.5555 - accuracy: 0.8372 - val_loss: 0.4773 - val_accuracy: 0.8652\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.5379 - accuracy: 0.8441 - val_loss: 0.5340 - val_accuracy: 0.8430\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.5360 - accuracy: 0.8456 - val_loss: 0.4829 - val_accuracy: 0.8610\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.5356 - accuracy: 0.8455 - val_loss: 0.4737 - val_accuracy: 0.8592\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.5300 - accuracy: 0.8455 - val_loss: 0.5196 - val_accuracy: 0.8532\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.5267 - accuracy: 0.8469 - val_loss: 0.4740 - val_accuracy: 0.8618\n",
            "Accuracy: 86.18%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4570 - accuracy: 0.8598 - val_loss: 0.3047 - val_accuracy: 0.9067\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.3290 - accuracy: 0.9015 - val_loss: 0.2830 - val_accuracy: 0.9170\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.3000 - accuracy: 0.9101 - val_loss: 0.2820 - val_accuracy: 0.9180\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.2929 - accuracy: 0.9120 - val_loss: 0.2816 - val_accuracy: 0.9178\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.2827 - accuracy: 0.9149 - val_loss: 0.2683 - val_accuracy: 0.9229\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.2747 - accuracy: 0.9166 - val_loss: 0.2516 - val_accuracy: 0.9271\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.2663 - accuracy: 0.9190 - val_loss: 0.2492 - val_accuracy: 0.9272\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.2604 - accuracy: 0.9208 - val_loss: 0.2595 - val_accuracy: 0.9249\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.2577 - accuracy: 0.9237 - val_loss: 0.2525 - val_accuracy: 0.9271\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.2534 - accuracy: 0.9233 - val_loss: 0.2452 - val_accuracy: 0.9308\n",
            "Accuracy: 93.08%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 3s 50us/step - loss: 0.7992 - accuracy: 0.7402 - val_loss: 0.4408 - val_accuracy: 0.8722\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.4623 - accuracy: 0.8678 - val_loss: 0.3984 - val_accuracy: 0.8859\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 47us/step - loss: 0.4100 - accuracy: 0.8818 - val_loss: 0.3512 - val_accuracy: 0.9000\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.3911 - accuracy: 0.8898 - val_loss: 0.3910 - val_accuracy: 0.8904\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 3s 49us/step - loss: 0.3773 - accuracy: 0.8930 - val_loss: 0.3325 - val_accuracy: 0.9102\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 0.3735 - accuracy: 0.8927 - val_loss: 0.3335 - val_accuracy: 0.9033\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.3657 - accuracy: 0.8945 - val_loss: 0.3307 - val_accuracy: 0.9059\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 48us/step - loss: 0.3642 - accuracy: 0.8950 - val_loss: 0.3339 - val_accuracy: 0.9031\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 3s 50us/step - loss: 0.3565 - accuracy: 0.8972 - val_loss: 0.3378 - val_accuracy: 0.9016\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 49us/step - loss: 0.3549 - accuracy: 0.8972 - val_loss: 0.3226 - val_accuracy: 0.9067\n",
            "Accuracy: 90.67%\n",
            "Train on 51000 samples, validate on 9000 samples\n",
            "Epoch 1/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.4486 - accuracy: 0.8622 - val_loss: 0.2365 - val_accuracy: 0.9274\n",
            "Epoch 2/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.2598 - accuracy: 0.9228 - val_loss: 0.1945 - val_accuracy: 0.9437\n",
            "Epoch 3/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.2275 - accuracy: 0.9328 - val_loss: 0.1875 - val_accuracy: 0.9446\n",
            "Epoch 4/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.2063 - accuracy: 0.9382 - val_loss: 0.1821 - val_accuracy: 0.9463\n",
            "Epoch 5/10\n",
            "51000/51000 [==============================] - 2s 45us/step - loss: 0.1919 - accuracy: 0.9429 - val_loss: 0.1782 - val_accuracy: 0.9466\n",
            "Epoch 6/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.1809 - accuracy: 0.9462 - val_loss: 0.1789 - val_accuracy: 0.9494\n",
            "Epoch 7/10\n",
            "51000/51000 [==============================] - 2s 46us/step - loss: 0.1719 - accuracy: 0.9479 - val_loss: 0.2094 - val_accuracy: 0.9413\n",
            "Epoch 8/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.1651 - accuracy: 0.9500 - val_loss: 0.1823 - val_accuracy: 0.9463\n",
            "Epoch 9/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.1585 - accuracy: 0.9519 - val_loss: 0.1713 - val_accuracy: 0.9527\n",
            "Epoch 10/10\n",
            "51000/51000 [==============================] - 2s 44us/step - loss: 0.1541 - accuracy: 0.9534 - val_loss: 0.1783 - val_accuracy: 0.9493\n",
            "Accuracy: 94.93%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAoFftJ7UxKE",
        "colab_type": "code",
        "outputId": "2b9f85c1-e51c-4407-997e-4bf1927e7146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#let's see the best accuracy\n",
        "print(\"best accuracy was \" + str(round(gp_result.fun *-100,2))+\"%.\")\n",
        "#and let's see the best values for: learning rate, number of layer\n",
        "#, number of hidden units and solver\n",
        "\n",
        "gp_result.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best accuracy was 94.93%.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09715134469818455, 1, 15, 'SGD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDAG2QFdWauj",
        "colab_type": "code",
        "outputId": "a55a0806-895b-42d7-b627-72634a985226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "#finally let's train the neural network with the optimal values and evaluate\n",
        "#the model on the test set\n",
        "gp_model = create_model(gp_result.x[0],gp_result.x[1],gp_result.x[2],gp_result.x[3])\n",
        "gp_model.fit(X_train,y_train, batch_size=10, epochs=10)\n",
        "gp_model.evaluate(X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.3775 - accuracy: 0.8861\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.2461 - accuracy: 0.9270\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.2157 - accuracy: 0.9364\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.1987 - accuracy: 0.9413\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.1866 - accuracy: 0.9446\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.1786 - accuracy: 0.9474\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.1746 - accuracy: 0.9485\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.1685 - accuracy: 0.9512\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1633 - accuracy: 0.9525\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.1626 - accuracy: 0.9524\n",
            "10000/10000 [==============================] - 0s 22us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.16214132775301113, 0.9524999856948853]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh8wI2z7Oeau",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Genetic Algoritms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1UCSdf7YcJL",
        "colab_type": "text"
      },
      "source": [
        "![Genetic Algorithms](https://miro.medium.com/max/638/1*exOoxtPvTQpCDcxg52bIWw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRJl_Z-ZS4a",
        "colab_type": "text"
      },
      "source": [
        "The hyperparameters tunning problem is an optimization problem, another technique we can use to solve this is using genetic algorithms. We start by selecting a set of random combinations of hyperparameters and we let them evolve following some rules and constraits to finlly converge to the optimal value.\n",
        "\n",
        "For doing this we use the DEAP library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4-CJ4xZRcRR",
        "colab_type": "code",
        "outputId": "3015b3af-180c-4c39-b3db-7eeaf352db2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "!pip install deap"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/eb/2bd0a32e3ce757fb26264765abbaedd6d4d3640d90219a513aeabd08ee2b/deap-1.3.1-cp36-cp36m-manylinux2010_x86_64.whl (157kB)\n",
            "\r\u001b[K     |██                              | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deap) (1.18.4)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFEfoIS-al1M",
        "colab_type": "code",
        "outputId": "551bda7c-a959-44aa-a2ba-b5d856c11c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy\n",
        "import elitism\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn import model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.utils.testing import ignore_warnings\n",
        "from math import floor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFNrF1Jran7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we start by creating a class with some function of interest for:\n",
        "#initializing the input with the MNIST dataset,\n",
        "#codify the hyperparameters into a form understood for the genetic algorithm,\n",
        "#calculate the accuracy of the model for a certain combination of hyperparameters.\n",
        "class MlpHyperparametersTest:\n",
        "    NUM_FOLDS = 5\n",
        "    def __init__(self, randomSeed):\n",
        "        self.randomSeed = randomSeed\n",
        "        self.initDataset()\n",
        "        self.kfold = model_selection.KFold(n_splits=self.NUM_FOLDS, random_state=self.randomSeed)\n",
        "\n",
        "    def initDataset(self):     \n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        self.X = numpy.vstack((x_train,x_test))\n",
        "        self.X = self.X / 255.0\n",
        "        self.X = self.X.reshape(self.X.shape[0], -1)\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "        y_test = y_test.reshape(-1, 1)\n",
        "        self.y = numpy.vstack((y_train,y_test))        \n",
        "        self.y = self.y.flatten()\n",
        "\n",
        "    def convertParams(self, params):\n",
        "        if round(params[1]) <= 0:\n",
        "            hiddenLayerSizes = round(params[0]),\n",
        "        elif round(params[2]) <= 0:\n",
        "            hiddenLayerSizes = (round(params[0]), round(params[1]))\n",
        "        elif round(params[3]) <= 0:\n",
        "            hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]))\n",
        "        else:\n",
        "            hiddenLayerSizes = (round(params[0]), round(params[1]), round(params[2]), round(params[3]))\n",
        "        learning_rate_init = numpy.power(10, params[4])\n",
        "        solver = ['sgd', 'adam'][floor(params[5])]        \n",
        "        return hiddenLayerSizes, learning_rate_init, solver\n",
        "        \n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def getAccuracy(self, params):\n",
        "        hiddenLayerSizes, learning_rate_init, solver = self.convertParams(params)\n",
        "        self.classifier = MLPClassifier(random_state=self.randomSeed,\n",
        "                                        hidden_layer_sizes=hiddenLayerSizes,\n",
        "                                        activation='relu',\n",
        "                                        solver=solver,\n",
        "                                        learning_rate_init=learning_rate_init,\n",
        "                                        max_iter = 10)\n",
        "        cv_results = model_selection.cross_val_score(self.classifier,\n",
        "                                                     self.X,\n",
        "                                                     self.y,\n",
        "                                                     cv=self.kfold,\n",
        "                                                     scoring='accuracy')\n",
        "        return cv_results.mean()\n",
        "\n",
        "    def formatParams(self, params):\n",
        "        hiddenLayerSizes, learning_rate_init, solver = self.convertParams(params)\n",
        "        return \"'hidden_layer_sizes'={}\\n \" \\\n",
        "               \"'solver'='{}'\\n \" \\\n",
        "               \"'learning_rate_init'='{}'\\n \" \\\n",
        "            .format(hiddenLayerSizes, solver, learning_rate_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXmgTTSKbqJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "#set the boundaries for the genetic code\n",
        "BOUNDS_LOW =  [ 5,  -5, -10, -20, -3, 0]\n",
        "BOUNDS_HIGH = [15,  15,  15,  15, -1, 1.999]\n",
        "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
        "#set some constant for the genetic algorithm\n",
        "POPULATION_SIZE = 7\n",
        "P_CROSSOVER = 0.8\n",
        "P_MUTATION = 0.2 \n",
        "MAX_GENERATIONS = 3\n",
        "HALL_OF_FAME_SIZE = 2\n",
        "CROWDING_FACTOR = 10.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qViDVRIpcAgB",
        "colab_type": "code",
        "outputId": "2a11b5d9-ffe7-40b3-a470-be1a3148d253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Here we define the characteristics algorithms and use the constant to \n",
        "#set the hyperparameters of the genetic algorithm (we are not tunning thoses lol!)\n",
        "\n",
        "def classificationAccuracy(individual):\n",
        "    return test.getAccuracy(individual),\n",
        "\n",
        "test = MlpHyperparametersTest(RANDOM_SEED)\n",
        "toolbox = base.Toolbox()\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "for i in range(NUM_OF_PARAMS):\n",
        "    toolbox.register(\"attribute_\" + str(i), random.uniform, BOUNDS_LOW[i], BOUNDS_HIGH[i])\n",
        "attributes = ()\n",
        "for i in range(NUM_OF_PARAMS):\n",
        "    attributes = attributes + (toolbox.__getattribute__(\"attribute_\" + str(i)),)\n",
        "toolbox.register(\"individualCreator\", tools.initCycle, creator.Individual, attributes, n=1)\n",
        "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.individualCreator)\n",
        "toolbox.register(\"evaluate\", classificationAccuracy)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
        "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=BOUNDS_LOW, up=BOUNDS_HIGH, eta=CROWDING_FACTOR)\n",
        "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=BOUNDS_LOW, up=BOUNDS_HIGH, eta=CROWDING_FACTOR, indpb=1.0/NUM_OF_PARAMS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5HE0K2ycibX",
        "colab_type": "code",
        "outputId": "a84f85b8-77b5-40d0-af4f-7573702711f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "#let put the algorithm to run and find the optimal values\n",
        "population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"max\", numpy.max)\n",
        "stats.register(\"avg\", numpy.mean)\n",
        "hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
        "population, logbook = elitism.eaSimpleWithElitism(population,\n",
        "                                                  toolbox,\n",
        "                                                  cxpb=P_CROSSOVER,\n",
        "                                                  mutpb=P_MUTATION,\n",
        "                                                  ngen=MAX_GENERATIONS,\n",
        "                                                  stats=stats,\n",
        "                                                  halloffame=hof,\n",
        "                                                  verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gen\tnevals\tmax     \tavg     \n",
            "0  \t7     \t0.934529\t0.785937\n",
            "1  \t2     \t0.934529\t0.927757\n",
            "2  \t4     \t0.934529\t0.932263\n",
            "3  \t2     \t0.934529\t0.93379 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQvAM7ufcsTt",
        "colab_type": "code",
        "outputId": "34b7f9c0-019e-4a57-d4b6-0804324d704d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "#let's print the solution with the higher accuracy\n",
        "#clarification for the number of layers: \n",
        "#(13,) would mean one layer with 14 units, (14, 14) would mean two layer of \n",
        "#14 units and (15, 15, 15) would mean three layers of 15 units\n",
        "print(\"- Best solution is: \\n\", test.formatParams(hof.items[0]), \"\\n 'accuracy' = \", test.getAccuracy(hof.items[0]))1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Best solution is: \n",
            " 'hidden_layer_sizes'=(12,)\n",
            " 'solver'='sgd'\n",
            " 'learning_rate_init'='0.019941421593918113'\n",
            "  \n",
            " 'accuracy' =  0.9345285714285714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ct6Ln5hTCC",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Hhyr3unfzy",
        "colab_type": "text"
      },
      "source": [
        "###Summary:\n",
        "\n",
        "####Search Grid: \n",
        "Accuracy: 95,18%. Hidden layers: 1. Hidden units: 15. Optimizer: SGD. Learning rate: 0.01.\n",
        "\n",
        "####Bayes Optimization: \n",
        "Accuracy: 94,93%. Hidden layers: 1. Hidden units: 15. Optimizer: SGD. Learning rate: 0.09715.\n",
        "\n",
        "####Genetic Algorithm: \n",
        "Accuracy: 93,45%. Hidden layers: 1. Hidden units: 12. Optimizer: SGD. Learning rate: 0.01994."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo-sqySEpDoZ",
        "colab_type": "text"
      },
      "source": [
        "###Analysis of results:\n",
        "\n",
        "Given these results, there are some pretty interest conclusions we can take and some clarifications we must do: In all the cases we get a very high accuracy, over 93%. However, one would expect the highest accuracies to be for the cases with Bayesian Optimization and Genetic Algorithms because thoses two are more sophisticated ones. \n",
        "\n",
        "This have a technical reason, when we implemented these algorithms we chose to use very little iterations for the sake of being able of simulate all in the same notebook. If we increase the number of calls in bayer optimization from 12 to 100 this will take a few hours but you will get an accuracy a little bit higher than 95% also. The same happens with the genetic algorithms case, if we increase the number of generations from 3 (this is very poor) to 20 you will get an accuracy higher than 95%. However the accuracy achieved in the Grid Search is very high so it is difficult to surpass.\n",
        "\n",
        "Finally, we obtained very similar results in the three cases. So, depending on the complexity of the system, the computational power and the design time, any of the three possibilities could be very useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqs01N-u_Sqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}