{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/agustinsilva447/CDL/blob/master/mnist_hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQM78DE1Eugu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkDW8TaqvMzY"
   },
   "outputs": [],
   "source": [
    "#función para graficar el accuracy en función de la cantidad de epochs\n",
    "def plot_accuracy_and_loss(history):\n",
    "\tplt.plot(history.history['accuracy'], label='accuracy')\n",
    "\tplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.ylim([0.5, 1])\n",
    "\tplt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tM0GeNFCvRUq"
   },
   "outputs": [],
   "source": [
    "#cargando el dataset MNIST desde keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#normalizando los valores de los pixels para que estén entre 0 y 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AetL_F0AvTqp"
   },
   "outputs": [],
   "source": [
    "# Todas estas son la posibles variables que se pueden autoconfigurar\n",
    "# sin embargo, solo vamos a trabajar con 3 para que las simulaciones\n",
    "# no lleven tanto tiempo. Vamos a trabajar con una red neuronal \n",
    "# con 2 hidden layers y optimizada utilizando stochastic gradient descent. \n",
    "# Vamos a tunear 3 parámetros: el learning rate, la cantidad de neuronas \n",
    "# por hidden layer y la cantidad de epochs.\n",
    "lr_ = [0.001, 0.01, 0.1]\t\t###tres casos usados\n",
    "hidden_units_ = [16, 32, 64]\t###tres casos usados\n",
    "epochs_ = [5, 10]\t\t\t\t##dos casos usados\n",
    "batch_size_ = 64\t\t\t\t#un solo caso usado\n",
    "momentum_ = 0.9\t\t\t\t\t#un solo caso usado\n",
    "decay_ = 1e-6\t\t\t\t\t#un solo caso usado\n",
    "hidden_layers_ = 2\t\t\t\t#un solo caso usado\n",
    "nesterov_ = True\t\t\t\t#un solo caso usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n228wXQHvWkT"
   },
   "outputs": [],
   "source": [
    "#inicialización de algunas variables\n",
    "test_loss = [None] * (len(hidden_units_) * len(epochs_) * len(lr_))\n",
    "test_acc =  [None] * (len(hidden_units_) * len(epochs_) * len(lr_))\n",
    "caso = 0\n",
    "max_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gfxBFP2kxbep",
    "outputId": "af9f12f0-88b3-4096-ea87-907462a596af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.3369 - accuracy: 0.5963 - val_loss: 0.6154 - val_accuracy: 0.8173\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.5157 - accuracy: 0.8490 - val_loss: 0.4158 - val_accuracy: 0.8782\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4026 - accuracy: 0.8847 - val_loss: 0.3591 - val_accuracy: 0.8932\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3567 - accuracy: 0.8974 - val_loss: 0.3281 - val_accuracy: 0.9050\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3275 - accuracy: 0.9068 - val_loss: 0.3053 - val_accuracy: 0.9136\n",
      "313/313 - 0s - loss: 0.3059 - accuracy: 0.9136\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4897 - accuracy: 0.8542 - val_loss: 0.2846 - val_accuracy: 0.9126\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2613 - accuracy: 0.9233 - val_loss: 0.2250 - val_accuracy: 0.9298\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2143 - accuracy: 0.9380 - val_loss: 0.1998 - val_accuracy: 0.9414\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9452 - val_loss: 0.1878 - val_accuracy: 0.9451\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1717 - accuracy: 0.9496 - val_loss: 0.1699 - val_accuracy: 0.9486\n",
      "313/313 - 0s - loss: 0.1703 - accuracy: 0.9486\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4138 - accuracy: 0.8752 - val_loss: 0.2778 - val_accuracy: 0.9203\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2719 - accuracy: 0.9212 - val_loss: 0.2675 - val_accuracy: 0.9232\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2460 - accuracy: 0.9305 - val_loss: 0.2435 - val_accuracy: 0.9303\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2342 - accuracy: 0.9356 - val_loss: 0.2274 - val_accuracy: 0.9366\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2214 - accuracy: 0.9389 - val_loss: 0.2208 - val_accuracy: 0.9395\n",
      "313/313 - 0s - loss: 0.2213 - accuracy: 0.9395\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.3298 - accuracy: 0.5675 - val_loss: 0.5726 - val_accuracy: 0.8431\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4782 - accuracy: 0.8647 - val_loss: 0.3885 - val_accuracy: 0.8846\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3790 - accuracy: 0.8925 - val_loss: 0.3376 - val_accuracy: 0.9010\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3394 - accuracy: 0.9025 - val_loss: 0.3096 - val_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3163 - accuracy: 0.9080 - val_loss: 0.2930 - val_accuracy: 0.9132\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2993 - accuracy: 0.9128 - val_loss: 0.2830 - val_accuracy: 0.9187\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.9169 - val_loss: 0.2733 - val_accuracy: 0.9208\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2759 - accuracy: 0.9189 - val_loss: 0.2641 - val_accuracy: 0.9244\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2663 - accuracy: 0.9221 - val_loss: 0.2554 - val_accuracy: 0.9246\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2580 - accuracy: 0.9238 - val_loss: 0.2489 - val_accuracy: 0.9281\n",
      "313/313 - 0s - loss: 0.2493 - accuracy: 0.9281\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.5143 - accuracy: 0.8448 - val_loss: 0.2982 - val_accuracy: 0.9092\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2644 - accuracy: 0.9226 - val_loss: 0.2303 - val_accuracy: 0.9291\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2220 - accuracy: 0.9367 - val_loss: 0.2124 - val_accuracy: 0.9376\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2012 - accuracy: 0.9411 - val_loss: 0.1945 - val_accuracy: 0.9425\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9453 - val_loss: 0.1843 - val_accuracy: 0.9458\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1764 - accuracy: 0.9480 - val_loss: 0.1808 - val_accuracy: 0.9457\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1658 - accuracy: 0.9518 - val_loss: 0.2127 - val_accuracy: 0.9363\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1604 - accuracy: 0.9529 - val_loss: 0.2001 - val_accuracy: 0.9392\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1539 - accuracy: 0.9543 - val_loss: 0.1804 - val_accuracy: 0.9472\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1494 - accuracy: 0.9561 - val_loss: 0.1726 - val_accuracy: 0.9487\n",
      "313/313 - 0s - loss: 0.1728 - accuracy: 0.9487\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3849 - accuracy: 0.8840 - val_loss: 0.2699 - val_accuracy: 0.9248\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2610 - accuracy: 0.9262 - val_loss: 0.2122 - val_accuracy: 0.9382\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2367 - accuracy: 0.9330 - val_loss: 0.2128 - val_accuracy: 0.9401\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2257 - accuracy: 0.9371 - val_loss: 0.2921 - val_accuracy: 0.9212\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2146 - accuracy: 0.9399 - val_loss: 0.2204 - val_accuracy: 0.9397\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2096 - accuracy: 0.9406 - val_loss: 0.2603 - val_accuracy: 0.9318\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2070 - accuracy: 0.9431 - val_loss: 0.2114 - val_accuracy: 0.9442\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9457 - val_loss: 0.2338 - val_accuracy: 0.9357\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9470 - val_loss: 0.2070 - val_accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9461 - val_loss: 0.2134 - val_accuracy: 0.9467\n",
      "313/313 - 0s - loss: 0.2140 - accuracy: 0.9467\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.1568 - accuracy: 0.6646 - val_loss: 0.4883 - val_accuracy: 0.8649\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4166 - accuracy: 0.8816 - val_loss: 0.3426 - val_accuracy: 0.9015\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.9045 - val_loss: 0.3021 - val_accuracy: 0.9125\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2948 - accuracy: 0.9145 - val_loss: 0.2707 - val_accuracy: 0.9227\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2706 - accuracy: 0.9216 - val_loss: 0.2555 - val_accuracy: 0.9265\n",
      "313/313 - 0s - loss: 0.2558 - accuracy: 0.9265\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4294 - accuracy: 0.8717 - val_loss: 0.2345 - val_accuracy: 0.9286\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9409 - val_loss: 0.1634 - val_accuracy: 0.9517\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1569 - accuracy: 0.9542 - val_loss: 0.1436 - val_accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1318 - accuracy: 0.9607 - val_loss: 0.1260 - val_accuracy: 0.9613\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1147 - accuracy: 0.9657 - val_loss: 0.1227 - val_accuracy: 0.9625\n",
      "313/313 - 0s - loss: 0.1230 - accuracy: 0.9625\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3037 - accuracy: 0.9091 - val_loss: 0.2171 - val_accuracy: 0.9380\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9462 - val_loss: 0.1871 - val_accuracy: 0.9453\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1692 - accuracy: 0.9522 - val_loss: 0.1820 - val_accuracy: 0.9506\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1607 - accuracy: 0.9543 - val_loss: 0.1580 - val_accuracy: 0.9549\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1490 - accuracy: 0.9582 - val_loss: 0.1678 - val_accuracy: 0.9554\n",
      "313/313 - 0s - loss: 0.1682 - accuracy: 0.9554\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.1599 - accuracy: 0.6575 - val_loss: 0.5379 - val_accuracy: 0.8451\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4599 - accuracy: 0.8673 - val_loss: 0.3805 - val_accuracy: 0.8888\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3648 - accuracy: 0.8948 - val_loss: 0.3246 - val_accuracy: 0.9077\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3217 - accuracy: 0.9071 - val_loss: 0.2943 - val_accuracy: 0.9153\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2945 - accuracy: 0.9148 - val_loss: 0.2730 - val_accuracy: 0.9228\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2733 - accuracy: 0.9203 - val_loss: 0.2560 - val_accuracy: 0.9267\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2553 - accuracy: 0.9266 - val_loss: 0.2406 - val_accuracy: 0.9319\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2403 - accuracy: 0.9306 - val_loss: 0.2311 - val_accuracy: 0.9326\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2271 - accuracy: 0.9348 - val_loss: 0.2176 - val_accuracy: 0.9377\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2152 - accuracy: 0.9374 - val_loss: 0.2061 - val_accuracy: 0.9409\n",
      "313/313 - 0s - loss: 0.2067 - accuracy: 0.9409\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4200 - accuracy: 0.8753 - val_loss: 0.2117 - val_accuracy: 0.9369\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9438 - val_loss: 0.1712 - val_accuracy: 0.9468\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1507 - accuracy: 0.9556 - val_loss: 0.1382 - val_accuracy: 0.9584\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1260 - accuracy: 0.9625 - val_loss: 0.1176 - val_accuracy: 0.9621\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1112 - accuracy: 0.9666 - val_loss: 0.1122 - val_accuracy: 0.9650\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.1000 - accuracy: 0.9702 - val_loss: 0.1119 - val_accuracy: 0.9638\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0909 - accuracy: 0.9726 - val_loss: 0.1023 - val_accuracy: 0.9682\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0836 - accuracy: 0.9747 - val_loss: 0.0976 - val_accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0780 - accuracy: 0.9759 - val_loss: 0.1098 - val_accuracy: 0.9661\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0732 - accuracy: 0.9773 - val_loss: 0.0975 - val_accuracy: 0.9701\n",
      "313/313 - 0s - loss: 0.0978 - accuracy: 0.9701\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2959 - accuracy: 0.9117 - val_loss: 0.1837 - val_accuracy: 0.9457\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9466 - val_loss: 0.1698 - val_accuracy: 0.9520\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1586 - accuracy: 0.9549 - val_loss: 0.2034 - val_accuracy: 0.9449\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1426 - accuracy: 0.9596 - val_loss: 0.1517 - val_accuracy: 0.9579\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1391 - accuracy: 0.9596 - val_loss: 0.1613 - val_accuracy: 0.9609\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1260 - accuracy: 0.9639 - val_loss: 0.1406 - val_accuracy: 0.9594\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1207 - accuracy: 0.9650 - val_loss: 0.1617 - val_accuracy: 0.9595\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.9662 - val_loss: 0.1779 - val_accuracy: 0.9515\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.9689 - val_loss: 0.1609 - val_accuracy: 0.9566\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1111 - accuracy: 0.9681 - val_loss: 0.1883 - val_accuracy: 0.9553\n",
      "313/313 - 0s - loss: 0.1889 - accuracy: 0.9553\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.9878 - accuracy: 0.7415 - val_loss: 0.4365 - val_accuracy: 0.8801\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3903 - accuracy: 0.8903 - val_loss: 0.3329 - val_accuracy: 0.9077\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3250 - accuracy: 0.9070 - val_loss: 0.2925 - val_accuracy: 0.9178\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.9170 - val_loss: 0.2687 - val_accuracy: 0.9230\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2659 - accuracy: 0.9230 - val_loss: 0.2468 - val_accuracy: 0.9284\n",
      "313/313 - 0s - loss: 0.2473 - accuracy: 0.9284\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3869 - accuracy: 0.8852 - val_loss: 0.1884 - val_accuracy: 0.9437\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1703 - accuracy: 0.9499 - val_loss: 0.1403 - val_accuracy: 0.9569\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9632 - val_loss: 0.1340 - val_accuracy: 0.9596\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1014 - accuracy: 0.9698 - val_loss: 0.0997 - val_accuracy: 0.9687\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0839 - accuracy: 0.9748 - val_loss: 0.0981 - val_accuracy: 0.9690\n",
      "313/313 - 0s - loss: 0.0984 - accuracy: 0.9690\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2512 - accuracy: 0.9242 - val_loss: 0.1302 - val_accuracy: 0.9616\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1322 - accuracy: 0.9613 - val_loss: 0.1395 - val_accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1102 - accuracy: 0.9674 - val_loss: 0.1236 - val_accuracy: 0.9661\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0949 - accuracy: 0.9716 - val_loss: 0.1264 - val_accuracy: 0.9663\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0876 - accuracy: 0.9733 - val_loss: 0.1403 - val_accuracy: 0.9625\n",
      "313/313 - 0s - loss: 0.1407 - accuracy: 0.9625\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.0031 - accuracy: 0.7323 - val_loss: 0.4380 - val_accuracy: 0.8829\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3916 - accuracy: 0.8902 - val_loss: 0.3296 - val_accuracy: 0.9059\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3249 - accuracy: 0.9072 - val_loss: 0.2943 - val_accuracy: 0.9145\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.9166 - val_loss: 0.2688 - val_accuracy: 0.9241\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2681 - accuracy: 0.9234 - val_loss: 0.2492 - val_accuracy: 0.9279\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2496 - accuracy: 0.9287 - val_loss: 0.2372 - val_accuracy: 0.9299\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2340 - accuracy: 0.9330 - val_loss: 0.2230 - val_accuracy: 0.9354\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2198 - accuracy: 0.9375 - val_loss: 0.2129 - val_accuracy: 0.9372\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9411 - val_loss: 0.2025 - val_accuracy: 0.9397\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9445 - val_loss: 0.1932 - val_accuracy: 0.9422\n",
      "313/313 - 0s - loss: 0.1936 - accuracy: 0.9422\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3806 - accuracy: 0.8881 - val_loss: 0.2023 - val_accuracy: 0.9395\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1729 - accuracy: 0.9484 - val_loss: 0.1406 - val_accuracy: 0.9583\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1287 - accuracy: 0.9624 - val_loss: 0.1204 - val_accuracy: 0.9621\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9700 - val_loss: 0.1055 - val_accuracy: 0.9679\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0851 - accuracy: 0.9740 - val_loss: 0.0941 - val_accuracy: 0.9704\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0723 - accuracy: 0.9781 - val_loss: 0.0957 - val_accuracy: 0.9719\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.0796 - val_accuracy: 0.9754\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0558 - accuracy: 0.9826 - val_loss: 0.0836 - val_accuracy: 0.9745\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0481 - accuracy: 0.9851 - val_loss: 0.0841 - val_accuracy: 0.9747\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0431 - accuracy: 0.9866 - val_loss: 0.0772 - val_accuracy: 0.9762\n",
      "313/313 - 0s - loss: 0.0775 - accuracy: 0.9762\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.2606 - accuracy: 0.9216 - val_loss: 0.1761 - val_accuracy: 0.9485\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1405 - accuracy: 0.9576 - val_loss: 0.1608 - val_accuracy: 0.9534\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1151 - accuracy: 0.9658 - val_loss: 0.1234 - val_accuracy: 0.9645\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0979 - accuracy: 0.9713 - val_loss: 0.1251 - val_accuracy: 0.9645\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0925 - accuracy: 0.9720 - val_loss: 0.1109 - val_accuracy: 0.9710\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0827 - accuracy: 0.9754 - val_loss: 0.1139 - val_accuracy: 0.9698\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0763 - accuracy: 0.9778 - val_loss: 0.1128 - val_accuracy: 0.9707\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0717 - accuracy: 0.9791 - val_loss: 0.1281 - val_accuracy: 0.9673\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0694 - accuracy: 0.9792 - val_loss: 0.1360 - val_accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0658 - accuracy: 0.9807 - val_loss: 0.1262 - val_accuracy: 0.9664\n",
      "313/313 - 0s - loss: 0.1266 - accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "# Barremos con los distintos valores de neuronas (3), epochs (2) \n",
    "# y learning rate (3). Para cada caso calculamos la accuracy y \n",
    "# nos quedamos con el caso con el valor más alto\n",
    "for i in range(len(hidden_units_)):\n",
    "  for j in range(len(epochs_)):\n",
    "    for k in range(len(lr_)):\n",
    "\t\t\t#modelo con 2 hidden layers\n",
    "      model = Sequential([\n",
    "      layers.Flatten(input_shape=(28, 28)),\n",
    "\t\t\tlayers.Dense(hidden_units_[i], activation='relu'),\n",
    "\t\t\tlayers.Dense(hidden_units_[i], activation='relu'),\n",
    "\t\t\tlayers.Dense(10, activation='softmax')\n",
    "      ])\n",
    "      \n",
    "\t\t\t#seteado de los parámetros del stochastic gradient descent\n",
    "      sgd = optimizers.SGD(lr=lr_[k], \n",
    "                  decay=decay_, \n",
    "                  momentum = momentum_, \n",
    "                  nesterov=nesterov_)\n",
    "      model.compile(optimizer=sgd, \n",
    "            loss='sparse_categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "      # entrenamiento de la red neuronal\n",
    "      history = model.fit(x_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs_[j], \n",
    "                      batch_size=batch_size_, \n",
    "                      validation_data=(x_test, y_test))\n",
    "                      \n",
    "      # evaluación de la red para la entrada del test set \n",
    "      # comparado con la verdaderas salidas\n",
    "      test_loss[caso], test_acc[caso] = model.evaluate(x_test,  \n",
    "                                                    y_test, verbose=2)  \n",
    "      \n",
    "      # nos quedamos con el valor de accuracy más alto\n",
    "      if test_acc[caso]>max_acc:\n",
    "        max_acc = test_acc[caso]\n",
    "        neu_opt = hidden_units_[i]\n",
    "        epo_opt = epochs_[j]\n",
    "        lra_opt = lr_[k]\n",
    "        hist_opt = history\n",
    "      caso += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2kLREXQzzEK7",
    "outputId": "dd1b56b0-fad4-4cdb-f0a9-4acff655b717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9136000275611877 . Loss: 0.30593693256378174\n",
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.9485999941825867 . Loss: 0.17033550143241882\n",
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9394999742507935 . Loss: 0.22129176557064056\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9280999898910522 . Loss: 0.2493029683828354\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9487000107765198 . Loss: 0.1728043556213379\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.9466999769210815 . Loss: 0.21403354406356812\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9265000224113464 . Loss: 0.2557702958583832\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.9624999761581421 . Loss: 0.1230340301990509\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9553999900817871 . Loss: 0.16820889711380005\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9409000277519226 . Loss: 0.20665831863880157\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9700999855995178 . Loss: 0.09779636561870575\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.955299973487854 . Loss: 0.18887673318386078\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9283999800682068 . Loss: 0.24731016159057617\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.968999981880188 . Loss: 0.09837350994348526\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9624999761581421 . Loss: 0.14068518579006195\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9422000050544739 . Loss: 0.19356900453567505\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9761999845504761 . Loss: 0.07748889923095703\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.9664000272750854 . Loss: 0.12661926448345184\n",
      "---------------------------------------------\n",
      "El Accuracy máximo es: 0.9761999845504761 . Para el caso con 64 cantidad de neuras, 10 epocs y un learning rate de 0.01\n"
     ]
    }
   ],
   "source": [
    "#mostrar resumen de todos los resultados y finalmente el caso óptimo\n",
    "caso = 0\n",
    "for i in range(len(hidden_units_)):\n",
    "\tfor j in range(len(epochs_)):\n",
    "\t\tfor k in range(len(lr_)):\n",
    "\t\t\tprint(\"Cantidad de neuronas:\",hidden_units_[i],\". Epochs:\",epochs_[j],\". Learning rate:\",lr_[k],\". Accuracy:\", test_acc[caso], \". Loss:\", test_loss[caso])\n",
    "\t\t\tcaso += 1\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"El Accuracy máximo es:\",max_acc,\". Para el caso con\",neu_opt,\"cantidad de neuras,\",epo_opt,\"epocs y un learning rate de\",lra_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "4FJqCI7rzLue",
    "outputId": "f15822ae-c465-4497-e31c-313ad879e7fa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSMSpEIiIRJwKgYhIxP1/DnLlSrOQFjsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy_and_loss(hist_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MdWU3UszPLe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOMjRsecwqfbmjD2qoio1ZJ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mnist_hyperparameters.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
