{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/agustinsilva447/CDL/blob/master/mnist_hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/alan/.local/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.27.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/alan/.local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow) (41.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/alan/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/alan/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQM78DE1Eugu",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkDW8TaqvMzY"
   },
   "outputs": [],
   "source": [
    "#función para graficar el accuracy en función de la cantidad de epochs\n",
    "def plot_accuracy_and_loss(history):\n",
    "\tplt.plot(history.history['accuracy'], label='accuracy')\n",
    "\tplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.ylim([0.5, 1])\n",
    "\tplt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tM0GeNFCvRUq"
   },
   "outputs": [],
   "source": [
    "#cargando el dataset MNIST desde keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#normalizando los valores de los pixels para que estén entre 0 y 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AetL_F0AvTqp"
   },
   "outputs": [],
   "source": [
    "# Todas estas son la posibles variables que se pueden autoconfigurar\n",
    "# sin embargo, solo vamos a trabajar con 3 para que las simulaciones\n",
    "# no lleven tanto tiempo. Vamos a trabajar con una red neuronal \n",
    "# con 2 hidden layers y optimizada utilizando stochastic gradient descent. \n",
    "# Vamos a tunear 3 parámetros: el learning rate, la cantidad de neuronas \n",
    "# por hidden layer y la cantidad de epochs.\n",
    "lr_ = [0.001, 0.01, 0.1]\t\t###tres casos usados\n",
    "hidden_units_ = [16, 32, 64]\t###tres casos usados\n",
    "epochs_ = [5, 10]\t\t\t\t##dos casos usados\n",
    "batch_size_ = 64\t\t\t\t#un solo caso usado\n",
    "momentum_ = 0.9\t\t\t\t\t#un solo caso usado\n",
    "decay_ = 1e-6\t\t\t\t\t#un solo caso usado\n",
    "hidden_layers_ = 2\t\t\t\t#un solo caso usado\n",
    "nesterov_ = True\t\t\t\t#un solo caso usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n228wXQHvWkT"
   },
   "outputs": [],
   "source": [
    "#inicialización de algunas variables\n",
    "test_loss = [None] * (len(hidden_units_) * len(epochs_) * len(lr_))\n",
    "test_acc =  [None] * (len(hidden_units_) * len(epochs_) * len(lr_))\n",
    "caso = 0\n",
    "max_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gfxBFP2kxbep",
    "outputId": "af9f12f0-88b3-4096-ea87-907462a596af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alan/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 1.3708 - acc: 0.5806 - val_loss: 0.6592 - val_acc: 0.8236\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.5156 - acc: 0.8552 - val_loss: 0.4048 - val_acc: 0.8817\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3846 - acc: 0.8907 - val_loss: 0.3420 - val_acc: 0.8992\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.3398 - acc: 0.9022 - val_loss: 0.3132 - val_acc: 0.9091\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3155 - acc: 0.9099 - val_loss: 0.2973 - val_acc: 0.9132\n",
      "10000/10000 - 0s - loss: 0.2973 - acc: 0.9132\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.4914 - acc: 0.8518 - val_loss: 0.2714 - val_acc: 0.9216\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2529 - acc: 0.9266 - val_loss: 0.2251 - val_acc: 0.9369\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2084 - acc: 0.9396 - val_loss: 0.1893 - val_acc: 0.9445\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1825 - acc: 0.9462 - val_loss: 0.1763 - val_acc: 0.9464\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1673 - acc: 0.9505 - val_loss: 0.1781 - val_acc: 0.9493\n",
      "10000/10000 - 0s - loss: 0.1781 - acc: 0.9493\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3760 - acc: 0.8862 - val_loss: 0.2904 - val_acc: 0.9173\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2559 - acc: 0.9273 - val_loss: 0.2372 - val_acc: 0.9351\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.2347 - acc: 0.9343 - val_loss: 0.2386 - val_acc: 0.9360\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2256 - acc: 0.9381 - val_loss: 0.2115 - val_acc: 0.9404\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2151 - acc: 0.9401 - val_loss: 0.2309 - val_acc: 0.9393\n",
      "10000/10000 - 0s - loss: 0.2309 - acc: 0.9393\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 1.2488 - acc: 0.6249 - val_loss: 0.5967 - val_acc: 0.8426\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.4876 - acc: 0.8674 - val_loss: 0.4046 - val_acc: 0.8916\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3857 - acc: 0.8910 - val_loss: 0.3474 - val_acc: 0.9047\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3447 - acc: 0.9025 - val_loss: 0.3217 - val_acc: 0.9113\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.3181 - acc: 0.9100 - val_loss: 0.3024 - val_acc: 0.9169\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2991 - acc: 0.9152 - val_loss: 0.2886 - val_acc: 0.9185\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.2836 - acc: 0.9197 - val_loss: 0.2734 - val_acc: 0.9234\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2699 - acc: 0.9239 - val_loss: 0.2612 - val_acc: 0.9248\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.2582 - acc: 0.9268 - val_loss: 0.2552 - val_acc: 0.9245\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.2481 - acc: 0.9303 - val_loss: 0.2421 - val_acc: 0.9302\n",
      "10000/10000 - 0s - loss: 0.2421 - acc: 0.9302\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.5294 - acc: 0.8394 - val_loss: 0.2787 - val_acc: 0.9199\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2541 - acc: 0.9262 - val_loss: 0.2193 - val_acc: 0.9341\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.2134 - acc: 0.9373 - val_loss: 0.2069 - val_acc: 0.9388\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1906 - acc: 0.9446 - val_loss: 0.1835 - val_acc: 0.9441\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.1760 - acc: 0.9476 - val_loss: 0.1716 - val_acc: 0.9499\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1643 - acc: 0.9512 - val_loss: 0.1773 - val_acc: 0.9449\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1541 - acc: 0.9535 - val_loss: 0.1721 - val_acc: 0.9492\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1470 - acc: 0.9559 - val_loss: 0.1681 - val_acc: 0.9495\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1418 - acc: 0.9574 - val_loss: 0.1690 - val_acc: 0.9532\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1365 - acc: 0.9585 - val_loss: 0.1673 - val_acc: 0.9498\n",
      "10000/10000 - 0s - loss: 0.1673 - acc: 0.9498\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.3716 - acc: 0.8897 - val_loss: 0.2834 - val_acc: 0.9183\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2556 - acc: 0.9287 - val_loss: 0.2566 - val_acc: 0.9302\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2265 - acc: 0.9363 - val_loss: 0.2178 - val_acc: 0.9378\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2159 - acc: 0.9402 - val_loss: 0.2111 - val_acc: 0.9432\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.2058 - acc: 0.9437 - val_loss: 0.2120 - val_acc: 0.9442\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.2026 - acc: 0.9455 - val_loss: 0.2094 - val_acc: 0.9441\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1979 - acc: 0.9455 - val_loss: 0.1963 - val_acc: 0.9489\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1909 - acc: 0.9475 - val_loss: 0.2585 - val_acc: 0.9346\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1865 - acc: 0.9488 - val_loss: 0.2153 - val_acc: 0.9465\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1823 - acc: 0.9495 - val_loss: 0.1934 - val_acc: 0.9491\n",
      "10000/10000 - 0s - loss: 0.1934 - acc: 0.9491\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 1.0973 - acc: 0.6952 - val_loss: 0.4914 - val_acc: 0.8709\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4296 - acc: 0.8803 - val_loss: 0.3562 - val_acc: 0.9007\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.3510 - acc: 0.9005 - val_loss: 0.3123 - val_acc: 0.9121\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.3145 - acc: 0.9103 - val_loss: 0.2876 - val_acc: 0.9178\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2900 - acc: 0.9171 - val_loss: 0.2749 - val_acc: 0.9221\n",
      "10000/10000 - 0s - loss: 0.2749 - acc: 0.9221\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.4198 - acc: 0.8746 - val_loss: 0.2199 - val_acc: 0.9340\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1976 - acc: 0.9408 - val_loss: 0.1796 - val_acc: 0.9438\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1552 - acc: 0.9542 - val_loss: 0.1532 - val_acc: 0.9538\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.1287 - acc: 0.9613 - val_loss: 0.1434 - val_acc: 0.9561\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1128 - acc: 0.9657 - val_loss: 0.1202 - val_acc: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1202 - acc: 0.9621\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.3010 - acc: 0.9109 - val_loss: 0.1979 - val_acc: 0.9416\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1941 - acc: 0.9440 - val_loss: 0.1797 - val_acc: 0.9456\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1730 - acc: 0.9499 - val_loss: 0.1960 - val_acc: 0.9458\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1592 - acc: 0.9538 - val_loss: 0.1556 - val_acc: 0.9586\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1501 - acc: 0.9574 - val_loss: 0.1682 - val_acc: 0.9559\n",
      "10000/10000 - 0s - loss: 0.1682 - acc: 0.9559\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 1.1933 - acc: 0.6512 - val_loss: 0.5092 - val_acc: 0.8607\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.4284 - acc: 0.8781 - val_loss: 0.3567 - val_acc: 0.8962\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3462 - acc: 0.8994 - val_loss: 0.3112 - val_acc: 0.9116\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3085 - acc: 0.9100 - val_loss: 0.2837 - val_acc: 0.9193\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2835 - acc: 0.9181 - val_loss: 0.2648 - val_acc: 0.9239\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2642 - acc: 0.9228 - val_loss: 0.2470 - val_acc: 0.9301\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.2486 - acc: 0.9276 - val_loss: 0.2355 - val_acc: 0.9317\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2356 - acc: 0.9321 - val_loss: 0.2273 - val_acc: 0.9347\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2240 - acc: 0.9354 - val_loss: 0.2142 - val_acc: 0.9374\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2145 - acc: 0.9386 - val_loss: 0.2077 - val_acc: 0.9394\n",
      "10000/10000 - 1s - loss: 0.2077 - acc: 0.9394\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.4244 - acc: 0.8765 - val_loss: 0.2240 - val_acc: 0.9322\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1984 - acc: 0.9430 - val_loss: 0.1944 - val_acc: 0.9404\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1507 - acc: 0.9561 - val_loss: 0.1465 - val_acc: 0.9548\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1252 - acc: 0.9627 - val_loss: 0.1193 - val_acc: 0.9634\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.1095 - acc: 0.9671 - val_loss: 0.1166 - val_acc: 0.9654\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0975 - acc: 0.9706 - val_loss: 0.1069 - val_acc: 0.9670\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0885 - acc: 0.9737 - val_loss: 0.1054 - val_acc: 0.9675\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0813 - acc: 0.9756 - val_loss: 0.1075 - val_acc: 0.9683\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0743 - acc: 0.9772 - val_loss: 0.1026 - val_acc: 0.9696\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0704 - acc: 0.9787 - val_loss: 0.0940 - val_acc: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0940 - acc: 0.9716\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3000 - acc: 0.9109 - val_loss: 0.2055 - val_acc: 0.9378\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1866 - acc: 0.9464 - val_loss: 0.2081 - val_acc: 0.9400\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1606 - acc: 0.9541 - val_loss: 0.1711 - val_acc: 0.9511\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1461 - acc: 0.9586 - val_loss: 0.1587 - val_acc: 0.9552\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.1379 - acc: 0.9612 - val_loss: 0.1500 - val_acc: 0.9598\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1285 - acc: 0.9633 - val_loss: 0.1759 - val_acc: 0.9559\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.1237 - acc: 0.9637 - val_loss: 0.1582 - val_acc: 0.9583\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1213 - acc: 0.9668 - val_loss: 0.1832 - val_acc: 0.9556\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1156 - acc: 0.9678 - val_loss: 0.1936 - val_acc: 0.9543\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1095 - acc: 0.9693 - val_loss: 0.1926 - val_acc: 0.9509\n",
      "10000/10000 - 0s - loss: 0.1926 - acc: 0.9509\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.9636 - acc: 0.7431 - val_loss: 0.4377 - val_acc: 0.8777\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3941 - acc: 0.8888 - val_loss: 0.3318 - val_acc: 0.9064\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3273 - acc: 0.9064 - val_loss: 0.2938 - val_acc: 0.9151\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2935 - acc: 0.9155 - val_loss: 0.2671 - val_acc: 0.9231\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2688 - acc: 0.9226 - val_loss: 0.2470 - val_acc: 0.9272\n",
      "10000/10000 - 0s - loss: 0.2470 - acc: 0.9272\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3908 - acc: 0.8836 - val_loss: 0.1931 - val_acc: 0.9435\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1761 - acc: 0.9475 - val_loss: 0.1553 - val_acc: 0.9515\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1306 - acc: 0.9615 - val_loss: 0.1218 - val_acc: 0.9640\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1058 - acc: 0.9686 - val_loss: 0.1129 - val_acc: 0.9641\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0891 - acc: 0.9737 - val_loss: 0.1006 - val_acc: 0.9690\n",
      "10000/10000 - 0s - loss: 0.1006 - acc: 0.9690\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2575 - acc: 0.9221 - val_loss: 0.1610 - val_acc: 0.9562\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1431 - acc: 0.9576 - val_loss: 0.1354 - val_acc: 0.9570\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.1148 - acc: 0.9669 - val_loss: 0.1258 - val_acc: 0.9605\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1025 - acc: 0.9696 - val_loss: 0.1484 - val_acc: 0.9590\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0903 - acc: 0.9739 - val_loss: 0.1369 - val_acc: 0.9627\n",
      "10000/10000 - 0s - loss: 0.1369 - acc: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 1.0087 - acc: 0.7302 - val_loss: 0.4277 - val_acc: 0.8818\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.3814 - acc: 0.8928 - val_loss: 0.3202 - val_acc: 0.9084\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3158 - acc: 0.9093 - val_loss: 0.2833 - val_acc: 0.9187\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2825 - acc: 0.9193 - val_loss: 0.2563 - val_acc: 0.9274\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2595 - acc: 0.9259 - val_loss: 0.2379 - val_acc: 0.9317\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2408 - acc: 0.9311 - val_loss: 0.2247 - val_acc: 0.9357\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2251 - acc: 0.9353 - val_loss: 0.2107 - val_acc: 0.9398\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2117 - acc: 0.9391 - val_loss: 0.1975 - val_acc: 0.9438\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1995 - acc: 0.9431 - val_loss: 0.1885 - val_acc: 0.9458\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.1890 - acc: 0.9458 - val_loss: 0.1809 - val_acc: 0.9488\n",
      "10000/10000 - 0s - loss: 0.1809 - acc: 0.9488\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3724 - acc: 0.8913 - val_loss: 0.2101 - val_acc: 0.9388\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.1743 - acc: 0.9478 - val_loss: 0.1420 - val_acc: 0.9586\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.1268 - acc: 0.9621 - val_loss: 0.1357 - val_acc: 0.9584\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1012 - acc: 0.9703 - val_loss: 0.1159 - val_acc: 0.9639\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0850 - acc: 0.9748 - val_loss: 0.0974 - val_acc: 0.9695\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0733 - acc: 0.9779 - val_loss: 0.0945 - val_acc: 0.9691\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.0642 - acc: 0.9806 - val_loss: 0.0876 - val_acc: 0.9707\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0566 - acc: 0.9832 - val_loss: 0.0834 - val_acc: 0.9743\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0511 - acc: 0.9838 - val_loss: 0.0954 - val_acc: 0.9695\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0451 - acc: 0.9863 - val_loss: 0.0829 - val_acc: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0829 - acc: 0.9741\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2545 - acc: 0.9221 - val_loss: 0.1688 - val_acc: 0.9501\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1388 - acc: 0.9588 - val_loss: 0.1506 - val_acc: 0.9558\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1167 - acc: 0.9651 - val_loss: 0.1272 - val_acc: 0.9638\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1048 - acc: 0.9699 - val_loss: 0.1154 - val_acc: 0.9683\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0989 - acc: 0.9712 - val_loss: 0.1155 - val_acc: 0.9686\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0852 - acc: 0.9745 - val_loss: 0.1262 - val_acc: 0.9672\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0813 - acc: 0.9765 - val_loss: 0.1351 - val_acc: 0.9661\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0719 - acc: 0.9788 - val_loss: 0.1307 - val_acc: 0.9707\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0694 - acc: 0.9797 - val_loss: 0.1786 - val_acc: 0.9600\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0756 - acc: 0.9787 - val_loss: 0.1551 - val_acc: 0.9645\n",
      "10000/10000 - 0s - loss: 0.1551 - acc: 0.9645\n"
     ]
    }
   ],
   "source": [
    "# Barremos con los distintos valores de neuronas (3), epochs (2) \n",
    "# y learning rate (3). Para cada caso calculamos la accuracy y \n",
    "# nos quedamos con el caso con el valor más alto\n",
    "for i in range(len(hidden_units_)):\n",
    "  for j in range(len(epochs_)):\n",
    "    for k in range(len(lr_)):\n",
    "\t\t\t#modelo con 2 hidden layers\n",
    "      model = Sequential([\n",
    "      layers.Flatten(input_shape=(28, 28)),\n",
    "\t\t\tlayers.Dense(hidden_units_[i], activation='relu'),\n",
    "\t\t\tlayers.Dense(hidden_units_[i], activation='relu'),\n",
    "\t\t\tlayers.Dense(10, activation='softmax')\n",
    "      ])\n",
    "      \n",
    "\t\t\t#seteado de los parámetros del stochastic gradient descent\n",
    "      sgd = optimizers.SGD(lr=lr_[k], \n",
    "                  decay=decay_, \n",
    "                  momentum = momentum_, \n",
    "                  nesterov=nesterov_)\n",
    "      model.compile(optimizer=sgd, \n",
    "            loss='sparse_categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "      # entrenamiento de la red neuronal\n",
    "      history = model.fit(x_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs_[j], \n",
    "                      batch_size=batch_size_, \n",
    "                      validation_data=(x_test, y_test))\n",
    "                      \n",
    "      # evaluación de la red para la entrada del test set \n",
    "      # comparado con la verdaderas salidas\n",
    "      test_loss[caso], test_acc[caso] = model.evaluate(x_test,  \n",
    "                                                    y_test, verbose=2)  \n",
    "      \n",
    "      # nos quedamos con el valor de accuracy más alto\n",
    "      if test_acc[caso]>max_acc:\n",
    "        max_acc = test_acc[caso]\n",
    "        neu_opt = hidden_units_[i]\n",
    "        epo_opt = epochs_[j]\n",
    "        lra_opt = lr_[k]\n",
    "        hist_opt = history\n",
    "      caso += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2kLREXQzzEK7",
    "outputId": "dd1b56b0-fad4-4cdb-f0a9-4acff655b717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9132 . Loss: 0.29725642626583576\n",
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.9493 . Loss: 0.1781372046675533\n",
      "Cantidad de neuronas: 16 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9393 . Loss: 0.23093445731922985\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9302 . Loss: 0.24212197411358358\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9498 . Loss: 0.16726445697247982\n",
      "Cantidad de neuronas: 16 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.9491 . Loss: 0.19335635280907154\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9221 . Loss: 0.2749239620089531\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.9621 . Loss: 0.12019431031756103\n",
      "Cantidad de neuronas: 32 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9559 . Loss: 0.16815929940771313\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9394 . Loss: 0.2077107172012329\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9716 . Loss: 0.09400753302085213\n",
      "Cantidad de neuronas: 32 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.9509 . Loss: 0.19262802281603217\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.001 . Accuracy: 0.9272 . Loss: 0.24703235473632812\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.01 . Accuracy: 0.969 . Loss: 0.10063384202234447\n",
      "Cantidad de neuronas: 64 . Epochs: 5 . Learning rate: 0.1 . Accuracy: 0.9627 . Loss: 0.13687111716326325\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.001 . Accuracy: 0.9488 . Loss: 0.18087707015424967\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.01 . Accuracy: 0.9741 . Loss: 0.08286524507146095\n",
      "Cantidad de neuronas: 64 . Epochs: 10 . Learning rate: 0.1 . Accuracy: 0.9645 . Loss: 0.15509047481043964\n",
      "---------------------------------------------\n",
      "El Accuracy máximo es: 0.9741 . Para el caso con 64 cantidad de neuras, 10 epocs y un learning rate de 0.01\n"
     ]
    }
   ],
   "source": [
    "#mostrar resumen de todos los resultados y finalmente el caso óptimo\n",
    "caso = 0\n",
    "for i in range(len(hidden_units_)):\n",
    "\tfor j in range(len(epochs_)):\n",
    "\t\tfor k in range(len(lr_)):\n",
    "\t\t\tprint(\"Cantidad de neuronas:\",hidden_units_[i],\". Epochs:\",epochs_[j],\". Learning rate:\",lr_[k],\". Accuracy:\", test_acc[caso], \". Loss:\", test_loss[caso])\n",
    "\t\t\tcaso += 1\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"El Accuracy máximo es:\",max_acc,\". Para el caso con\",neu_opt,\"cantidad de neuras,\",epo_opt,\"epocs y un learning rate de\",lra_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "4FJqCI7rzLue",
    "outputId": "f15822ae-c465-4497-e31c-313ad879e7fa"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-89682925bd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_accuracy_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e504f824dfa9>\u001b[0m in \u001b[0;36mplot_accuracy_and_loss\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#función para graficar el accuracy en función de la cantidad de epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_accuracy_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "plot_accuracy_and_loss(hist_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MdWU3UszPLe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOMjRsecwqfbmjD2qoio1ZJ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mnist_hyperparameters.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
